{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70531c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan todas las librerías que vamos a usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd89e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    just had a real good moment. i missssssssss hi...\n",
       "1           is reading manga  http://plurk.com/p/mzp1e\n",
       "2    @comeagainjen http://twitpic.com/2y2lx - http:...\n",
       "3    @lapcat Need to send 'em to my accountant tomo...\n",
       "4        ADD ME ON MYSPACE!!!  myspace.com/LookThunder\n",
       "Name: message to examine, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lectura de tablas de datos con panda\n",
    "data = pd.read_csv('sentiment_tweets3.csv')\n",
    "dataset = data['message to examine']  \n",
    "dataset[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2767144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  need to send 'em to my accountant   . 23 or 24¯¿c possible today. nice \n",
      "{'am', 'mustn', 'needn', 'of', 'which', 'i', \"you'll\", 'not', 'when', 'same', 'we', 'if', 'ourselves', 'she', 'don', 'o', 'his', 'too', 'both', 'what', 'by', \"it's\", 'hasn', 'were', 'from', 'these', 'won', \"isn't\", 'few', 'again', 'and', 'y', 'themselves', 'whom', 'up', 'isn', 'me', 'about', 'an', 'our', 'no', 'yours', 'aren', 'between', 'on', \"you're\", 'own', 'as', 'himself', 'it', \"hasn't\", \"weren't\", 'have', \"that'll\", 'being', 'off', 'mightn', 'should', 'theirs', 'you', 'for', 'where', 'before', \"wasn't\", 'herself', 'd', 'then', 'after', 'yourselves', 'wouldn', 'hers', 'how', 'been', 'down', 'be', 'a', 'each', 'couldn', 'but', 'here', 'further', 'such', 'wasn', 't', 'their', 'those', 'during', 'against', 'over', 'any', 'all', 'other', 'll', \"didn't\", 'shouldn', \"wouldn't\", \"shan't\", 'through', 'ma', 'than', 'my', 'are', 'nor', 'shan', \"doesn't\", \"haven't\", 'while', \"don't\", 'they', 'now', 'itself', 'doesn', 'can', \"hadn't\", \"needn't\", 'just', 'to', 'so', 'haven', 'them', 'more', 'weren', \"she's\", 'its', 'at', 'myself', 'most', 've', 'or', 'because', 'that', \"couldn't\", \"aren't\", 'him', 'has', 'above', 'in', 'very', 'once', 'with', 're', 'will', 'does', 'doing', \"won't\", 'her', 'the', 'didn', \"you've\", 'is', 'only', 's', 'below', \"should've\", 'your', 'hadn', 'yourself', 'do', 'who', 'm', \"mustn't\", 'under', 'out', 'ours', 'was', 'until', 'did', 'he', 'into', 'some', 'had', 'there', 'why', \"mightn't\", 'this', \"shouldn't\", 'ain', \"you'd\", 'having'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "string = \"@lapcat myspace.com/lookthund Need to send 'em to my accountant #tomo http://www.youtube.com/watch?v=zoGfqvh2ME8 <Emoji: Person with folded hands>. 23 or 24Ã¯Â¿Â½C possible today. Nice \"\n",
    "patron = re.compile(r'\\b\\S+\\.com\\S+\\b')\n",
    "new_string = re.sub(r'Ã|Â|â|½|<Emoji:\\s*[^>]+>|https?://\\S+|www\\.\\S+|\\b\\S+\\.com\\S+\\b|[@#]\\w+',\"\", string)\n",
    "new = new_string.lower()\n",
    "print(new)\n",
    "\n",
    "stopW = set(stopwords.words('english'))\n",
    "print(stopW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#string = \"@comeagainjen http://twitpic.com/2y2lx - HOLA http://www.youtube.com/watch?v=zoGfqvh2ME8\"\n",
    "string=\"@lapcat Need to doing send 'em to my accountant #tomo http://www.youtube.com/watch?v=zoGfqvh2ME8 <Emoji: Person with folded hands>. 23 or 24Ã¯Â¿Â½C possible today. Nice \"\n",
    "new_string = re.sub(r'Ã|Â|â|½|<Emoji:\\s*[^>]+>|https?://\\S+|www\\.\\S+|[@#]\\w+',\"\", string)\n",
    "new = new_string.lower()\n",
    "tokenizer = TweetTokenizer()\n",
    "sep = tokenizer.tokenize(new)\n",
    "\n",
    "stopW = set(stopwords.words('english'))\n",
    "#print(stopW)\n",
    "filtered_sentence = [word for word in sep if not word in stopW]\n",
    "\n",
    "#filtered_sentence = []\n",
    "#for word in sep:\n",
    "#    if word not in stopwords:\n",
    "#        filtered_sentence.append(word)\n",
    "\n",
    "#print(filtered_sentence)\n",
    "lemat = SnowballStemmer('english')\n",
    "stems = [lemat.stem(word) for word in filtered_sentence]\n",
    "\n",
    "print(\"Tweet original:\", string)\n",
    "print(\"Texto sin stopwords:\", ' '.join(filtered_sentence))\n",
    "print(\"Texto stemmizado:\", ' '.join(stems))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "text = \"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\n",
    "tokenizer = TweetTokenizer()\n",
    "sep = tokenizer.tokenize(text)\n",
    "\n",
    "stems = [spanishstemmer.stem(word) for word in sep]\n",
    "print(stems)\n",
    "print(\": [text’, ‘pid’, ‘grit’, ‘proces’, ‘cant’, ‘cant’, ‘cant’, ‘cant’, ‘cant’, ‘cant’.]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "def limpiar_texto(data):\n",
    "    new_data=[]\n",
    "    for line in range(0,len(data)): \n",
    "        new_line = re.sub(r'Ã|Â|â|ï|½|<Emoji:\\s*[^>]+>|https?://\\S+|www\\.\\S+|[@#]\\w+',\"\", data[line])\n",
    "        new2 = new_line.lower()\n",
    "        tokenizer = TweetTokenizer()\n",
    "        sep = tokenizer.tokenize(new2)\n",
    "        new_data.append(sep)\n",
    "        stopwords = set(stopwords.words('english'))\n",
    "        #print(stopwords)\n",
    "        \n",
    "        for lista in range(0,len(new_data)):\n",
    "            filtered_sentence = [word for word in sep if not word in stopwords]\n",
    "            #print(filtered_sentence)\n",
    "\n",
    "        \n",
    "        \n",
    "    print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e94645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "def limpiar_texto(data):\n",
    "\n",
    "    for line in range(0,len(data)):\n",
    "        #print(line)\n",
    "        ## Eliminación de carácteres raros, URL, hashtags, menciones y emojis. \n",
    "        # Aparecían muchos carácteres hacia el final del dataset como . Buscando como podía eliminarlo, conseguí encontrar una\n",
    "        # manera de recopilar esos puntos junto a otros caracteres como â a traves de: [^\\x20-\\x7E]. \n",
    "        new_line = re.sub(r'Ã|Â|[^\\x20-\\x7E]|ì|ë|°|ï|½|<Emoji:\\s*[^>]+>|https?://\\S+|www\\.\\S+|\\b\\S+\\.com\\S+\\b|[@#]\\w+|@|¦|¢',\"\", data[line])\n",
    "        new2 = new_line.lower()\n",
    "        tokenizer = TweetTokenizer()\n",
    "        sep = tokenizer.tokenize(new2)\n",
    "        #print(sep)\n",
    "        stopW = set(stopwords.words('english'))\n",
    "        #print(stopW)\n",
    "        \n",
    "        for lista in sep:\n",
    "            filtered_sentence = [word for word in sep if not word in stopW]\n",
    "            #print(filtered_sentence)\n",
    "            \n",
    "#        filtered_sentence = []\n",
    "#        for word in sep:\n",
    "#            if word not in stopW:\n",
    "#                filtered_sentence.append(word)\n",
    "\n",
    "            lemat = SnowballStemmer('english')\n",
    "            stems = [lemat.stem(word) for word in filtered_sentence]\n",
    "        #print(stems)\n",
    "        final_text = ' '.join(stems)\n",
    "        #print(final_text)\n",
    "        data[line] == final_text\n",
    "        \n",
    "    #print(len(data))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8bfcee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   real good moment . missssssssss much ,\n",
       "1                                               read manga\n",
       "2                                                        -\n",
       "3        need send ' em account tomorrow . odd , even r...\n",
       "4                                         add myspac ! ! !\n",
       "                               ...                        \n",
       "10309    depress g herbo mood , i'm done stress peopl d...\n",
       "10310    depress succumb brain make feel like never eno...\n",
       "10311     ketamin nasal spray show promis depress , suicid\n",
       "10312         dont mistak bad day depress ! everyon ' em !\n",
       "10313                                                    0\n",
       "Name: message to examine, Length: 10314, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limpiar_texto(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6513e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[0:5]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92ad93",
   "metadata": {},
   "source": [
    "### ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "line= \"real good moment . missssssssss much ,\"\n",
    "text = TextBlob(line)\n",
    "sentiment_polarity = text.sentiment.polarity\n",
    "print(sentiment_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificador(data):\n",
    "    for line in range(0,len(data)):\n",
    "        text = TextBlob(data[line])\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a3ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def clasificador(data):\n",
    "    labels = []\n",
    "    for line in range(0,len(data)):\n",
    "        text = TextBlob(data[line])\n",
    "        sentiment_polarity = text.sentiment.polarity\n",
    "        #print(sentiment_polarity, text)\n",
    "        #print(line)\n",
    "        if -1 <= sentiment_polarity <= -0.8:\n",
    "            label = \"Hater\"\n",
    "        elif -0.8 < sentiment_polarity < 0:\n",
    "            label = \"Molesto\"\n",
    "        elif sentiment_polarity == 0:\n",
    "            label = \"Neutro\"\n",
    "        elif 0 < sentiment_polarity < 0.8:\n",
    "            label = \"Contento\"\n",
    "        elif 0.8 <= sentiment_polarity <= 1:\n",
    "            label= \"Muy feliz\"\n",
    "        labels.append(label)\n",
    "        \n",
    "#        print(line, text, sentiment_polarity, label)\n",
    "#    print(labels)\n",
    "#    print(len(labels))\n",
    "#    print(len(data),len(labels))\n",
    "    data['label'] = labels\n",
    "#    data.insert(loc=1, column='label', value=labels)\n",
    "#    print(data[0:5])\n",
    "\n",
    "    \n",
    "#    print(data[0:5])\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870719f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clasificador(dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "clasificador(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
