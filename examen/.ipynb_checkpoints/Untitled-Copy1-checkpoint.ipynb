{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46ae1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan todas las librerías que vamos a usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize#Se cargan todas las librerías que vamos a usar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4524e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message to examine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  message to examine\n",
       "0  just had a real good moment. i missssssssss hi...\n",
       "1         is reading manga  http://plurk.com/p/mzp1e\n",
       "2  @comeagainjen http://twitpic.com/2y2lx - http:...\n",
       "3  @lapcat Need to send 'em to my accountant tomo...\n",
       "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lectura de tablas de datos con panda\n",
    "data = pd.read_csv('sentiment_tweets3.csv')\n",
    "dataset = data[['message to examine']]\n",
    "dataset[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a915e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "def limpiar_texto(data):\n",
    "\n",
    "    for index, row in data.iterrows():  \n",
    "        text = row['message to examine']\n",
    "        ## Eliminación de carácteres raros, URL, hashtags, menciones y emojis. \n",
    "        # Aparecían muchos carácteres hacia el final del dataset como . Buscando como podía eliminarlo, conseguí encontrar una\n",
    "        # manera de recopilar esos puntos junto a otros caracteres como â a traves de: [^\\x20-\\x7E]. \n",
    "        new_line = re.sub(r'Ã|Â|[^\\x20-\\x7E]|ì|ë|°|ï|½|<Emoji:\\s*[^>]+>|https?://\\S+|www\\.\\S+|\\b\\S+\\.com\\S+\\b|[@#]\\w+|@|¦|¢',\"\", data[text])\n",
    "        new2 = new_line.lower()\n",
    "        tokenizer = TweetTokenizer()\n",
    "        sep = tokenizer.tokenize(new2)\n",
    "        #print(sep)\n",
    "        stopW = set(stopwords.words('english'))\n",
    "        #print(stopW)\n",
    "        \n",
    "        for lista in sep:\n",
    "            filtered_sentence = [word for word in sep if not word in stopW]\n",
    "            #print(filtered_sentence)\n",
    "            \n",
    "#        filtered_sentence = []\n",
    "#        for word in sep:\n",
    "#            if word not in stopW:\n",
    "#                filtered_sentence.append(word)\n",
    "\n",
    "            lemat = SnowballStemmer('english')\n",
    "            stems = [lemat.stem(word) for word in filtered_sentence]\n",
    "        #print(stems)\n",
    "        final_text = ' '.join(stems)\n",
    "        #print(final_text)\n",
    "        data[text] = final_text \n",
    "        \n",
    "    #print(len(data))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98834c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'just had a real good moment. i missssssssss him so much, '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'just had a real good moment. i missssssssss him so much, '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m limpiar_texto(dataset)\n",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m, in \u001b[0;36mlimpiar_texto\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage to examine\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m## Eliminación de carácteres raros, URL, hashtags, menciones y emojis. \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Aparecían muchos carácteres hacia el final del dataset como . Buscando como podía eliminarlo, conseguí encontrar una\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# manera de recopilar esos puntos junto a otros caracteres como â a traves de: [^\\x20-\\x7E]. \u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m new_line \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÃ|Â|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mx20-\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mx7E]|ì|ë|°|ï|½|<Emoji:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*[^>]+>|https?://\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.com\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb|[@#]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|@|¦|¢\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[text])\n\u001b[0;32m     13\u001b[0m new2 \u001b[38;5;241m=\u001b[39m new_line\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m TweetTokenizer()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'just had a real good moment. i missssssssss him so much, '"
     ]
    }
   ],
   "source": [
    "limpiar_texto(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9d4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def clasificador(data):\n",
    "    labels = []\n",
    "    for line in range(0,len(data)):\n",
    "        text = TextBlob(data[line])\n",
    "        sentiment_polarity = text.sentiment.polarity\n",
    "        #print(sentiment_polarity, text)\n",
    "        \n",
    "        if -1 <= sentiment_polarity <= -0.8:\n",
    "            label = \"Hater\"\n",
    "        elif -0.8 < sentiment_polarity < 0:\n",
    "            label = \"Molesto\"\n",
    "        elif sentiment_polarity == 0:\n",
    "            label = \"Neutro\"\n",
    "        elif 0 < sentiment_polarity < 0.8:\n",
    "            label = \"Contento\"\n",
    "        elif 0.8 <= sentiment_polarity <= 1:\n",
    "            label= \"Muy feliz\"\n",
    "        labels.append(label)\n",
    "        \n",
    "#        print(line, text, sentiment_polarity, label)\n",
    "#    print(labels)\n",
    "#    print(len(labels))\n",
    "#    print(len(data),len(labels))\n",
    "    data = pd.DataFrame(data)\n",
    "#    print(data[0:5])\n",
    "    data['label'] = labels\n",
    "#    data.insert(loc=1, column='label', value=labels)\n",
    "    \n",
    "#    print(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687562bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>real good moment . missssssssss much ,</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read manga</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need send ' em account tomorrow . odd , even r...</td>\n",
       "      <td>Molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>add myspac ! ! !</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10309</th>\n",
       "      <td>depress g herbo mood , i'm done stress peopl d...</td>\n",
       "      <td>Molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10310</th>\n",
       "      <td>depress succumb brain make feel like never eno...</td>\n",
       "      <td>Molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10311</th>\n",
       "      <td>ketamin nasal spray show promis depress , suicid</td>\n",
       "      <td>Molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>dont mistak bad day depress ! everyon ' em !</td>\n",
       "      <td>Molesto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10313</th>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      message to examine     label\n",
       "0                 real good moment . missssssssss much ,  Contento\n",
       "1                                             read manga    Neutro\n",
       "2                                                      -    Neutro\n",
       "3      need send ' em account tomorrow . odd , even r...   Molesto\n",
       "4                                       add myspac ! ! !    Neutro\n",
       "...                                                  ...       ...\n",
       "10309  depress g herbo mood , i'm done stress peopl d...   Molesto\n",
       "10310  depress succumb brain make feel like never eno...   Molesto\n",
       "10311   ketamin nasal spray show promis depress , suicid   Molesto\n",
       "10312       dont mistak bad day depress ! everyon ' em !   Molesto\n",
       "10313                                                  0    Neutro\n",
       "\n",
       "[10314 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificador(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135f3a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               real good moment . missssssssss much ,\n",
       "1                                           read manga\n",
       "2                                                    -\n",
       "3    need send ' em account tomorrow . odd , even r...\n",
       "4                                     add myspac ! ! !\n",
       "Name: message to examine, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52618d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = data[\"message to examine\"]\n",
    "#Vectorización del texto del atributo con el codificador CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "#Conversión de los atributos en valores numéricos\n",
    "atributos = vectorizer.fit_transform(atributos)\n",
    "\n",
    "#Lo mismo para el objetivo\n",
    "objetivo = data[\"label\"]\n",
    "objetivo = vectorizer.fit_transform(objetivo)\n",
    "\n",
    "#Separación del conjunto de entrenamiento y de prueba\n",
    "(atributos_entrenamiento, atributos_prueba,\n",
    " objetivo_entrenamiento, objetivo_prueba) = train_test_split(\n",
    "        atributos, objetivo,\n",
    "        random_state=12345,\n",
    "        test_size=.2,\n",
    "        stratify=objetivo)\n",
    "\n",
    "#Entrenamiento del modelo de Naive Bayes usando la instancia MultinomialNB\n",
    "sentiment_detector = MultinomialNB(alpha=1.0)  # alpha es el parámetro de suavizado\n",
    "sentiment_detector.fit(atributos_entrenamiento, objetivo_entrenamiento)\n",
    "\n",
    "#Predicciones con el conjunto de prueba\n",
    "predicciones = sentiment_detector.predict(atributos_prueba)\n",
    "#Cálculo de la precisión del modelo\n",
    "precision = sentiment_detector.score(atributos_prueba, objetivo_prueba)\n",
    "print(\"La precisión del modelo desarrollado es\", precision*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d107e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
