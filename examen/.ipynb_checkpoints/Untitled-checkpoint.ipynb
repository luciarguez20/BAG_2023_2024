{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46ae1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan todas las librerías que vamos a usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize#Se cargan todas las librerías que vamos a usar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4524e556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    just had a real good moment. i missssssssss hi...\n",
       "1           is reading manga  http://plurk.com/p/mzp1e\n",
       "2    @comeagainjen http://twitpic.com/2y2lx - http:...\n",
       "3    @lapcat Need to send 'em to my accountant tomo...\n",
       "4        ADD ME ON MYSPACE!!!  myspace.com/LookThunder\n",
       "Name: message to examine, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lectura de tablas de datos con panda\n",
    "data = pd.read_csv('sentiment_tweets3.csv')\n",
    "dataset = data['message to examine']\n",
    "dataset[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a915e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "def limpiar_texto(data):\n",
    "\n",
    "    for line in range(0,(len(data)-1)): \n",
    "\n",
    "        ## Eliminación de carácteres raros, URL, hashtags, menciones y emojis. \n",
    "        # Aparecían muchos carácteres hacia el final del dataset como . Buscando como podía eliminarlo, conseguí encontrar una\n",
    "        # manera de recopilar esos puntos junto a otros caracteres como â a traves de: [^\\x20-\\x7E]. \n",
    "        new_line = re.sub(r'Ã|Â|[^\\x20-\\x7E]|ì|ë|°|ï|½|<Emoji:\\s*[^>]+>|https?://\\S+|www\\.\\S+|\\b\\S+\\.com\\S+\\b|[@#]\\w+|@|¦|¢',\"\", data[line])\n",
    "        new2 = new_line.lower()\n",
    "        tokenizer = TweetTokenizer()\n",
    "        sep = tokenizer.tokenize(new2)\n",
    "        #print(sep)\n",
    "        stopW = set(stopwords.words('english'))\n",
    "        #print(stopW)\n",
    "        \n",
    "        for lista in sep:\n",
    "            filtered_sentence = [word for word in sep if not word in stopW]\n",
    "            #print(filtered_sentence)\n",
    "            \n",
    "#        filtered_sentence = []\n",
    "#        for word in sep:\n",
    "#            if word not in stopW:\n",
    "#                filtered_sentence.append(word)\n",
    "\n",
    "            lemat = SnowballStemmer('english')\n",
    "            stems = [lemat.stem(word) for word in filtered_sentence]\n",
    "        #print(stems)\n",
    "        final_text = ' '.join(stems)\n",
    "        #print(final_text)\n",
    "        data[line] == final_text \n",
    "        \n",
    "    #print(len(data))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98834c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        just had a real good moment. i missssssssss hi...\n",
       "1               is reading manga  http://plurk.com/p/mzp1e\n",
       "2        @comeagainjen http://twitpic.com/2y2lx - http:...\n",
       "3        @lapcat Need to send 'em to my accountant tomo...\n",
       "4            ADD ME ON MYSPACE!!!  myspace.com/LookThunder\n",
       "                               ...                        \n",
       "10309    No Depression by G Herbo is my mood from now o...\n",
       "10310    What do you do when depression succumbs the br...\n",
       "10311    Ketamine Nasal Spray Shows Promise Against Dep...\n",
       "10312    dont mistake a bad day with depression! everyo...\n",
       "10313                                                    0\n",
       "Name: message to examine, Length: 10314, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limpiar_texto(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c9d4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def clasificador(data):\n",
    "    labels = []\n",
    "    for line in range(0,len(data)):\n",
    "        text = TextBlob(data[line])\n",
    "        sentiment_polarity = text.sentiment.polarity\n",
    "        #print(sentiment_polarity, text)\n",
    "        \n",
    "        if -1 <= sentiment_polarity <= -0.8:\n",
    "            label = \"Hater\"\n",
    "        elif -0.8 < sentiment_polarity < 0:\n",
    "            label = \"Molesto\"\n",
    "        elif sentiment_polarity == 0:\n",
    "            label = \"Neutro\"\n",
    "        elif 0 < sentiment_polarity < 0.8:\n",
    "            label = \"Contento\"\n",
    "        elif 0.8 <= sentiment_polarity <= 1:\n",
    "            label= \"Muy feliz\"\n",
    "        labels.append(label)\n",
    "        #print(text, sentiment_polarity, label)\n",
    "#    print(labels)\n",
    "#    print(len(labels))\n",
    "    print(len(data),len(labels))\n",
    "#    data['label'] = labels\n",
    "#    data==data.assign(labels)\n",
    "#    print(data[0:5])\n",
    "\n",
    "#    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687562bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10314",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 10314",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clasificador(dataset)\n",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m, in \u001b[0;36mclasificador\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m----> 5\u001b[0m     text \u001b[38;5;241m=\u001b[39m TextBlob(data[line])\n\u001b[0;32m      6\u001b[0m     sentiment_polarity \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#print(sentiment_polarity, text)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 10314"
     ]
    }
   ],
   "source": [
    "clasificador(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529b7999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  message to examine\n",
      "0  just had a real good moment. i missssssssss hi...\n",
      "1         is reading manga  http://plurk.com/p/mzp1e\n",
      "2  @comeagainjen http://twitpic.com/2y2lx - http:...\n",
      "3  @lapcat Need to send 'em to my accountant tomo...\n",
      "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder\n",
      "                                  message to examine  nueva_columna\n",
      "0  just had a real good moment. i missssssssss hi...              1\n",
      "1         is reading manga  http://plurk.com/p/mzp1e              2\n",
      "2  @comeagainjen http://twitpic.com/2y2lx - http:...              3\n",
      "3  @lapcat Need to send 'em to my accountant tomo...              4\n",
      "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder              5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Supongamos que df es tu DataFrame existente\n",
    "data = {'message to examine': ['just had a real good moment. i missssssssss hi...',\n",
    "                               'is reading manga  http://plurk.com/p/mzp1e',\n",
    "                               '@comeagainjen http://twitpic.com/2y2lx - http://example.com',\n",
    "                               '@lapcat Need to send \\'em to my accountant tomo...',\n",
    "                               'ADD ME ON MYSPACE!!!  myspace.com/LookThunder']}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "# Añadir una nueva columna llamada 'nueva_columna'\n",
    "datos=[1, 2, 3, 4, 5]\n",
    "df['nueva_columna'] = datos\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
